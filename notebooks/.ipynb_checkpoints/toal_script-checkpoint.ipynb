{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "import selenium.webdriver.support.ui as ui\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import selenium.webdriver.support.expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import requests\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "import random\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup, Tag, NavigableString\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "\n",
    "\n",
    "def google_scholar_scraper(start_year=2009,end_year=datetime.datetime.now().year):\n",
    "    show_more_button_path = '/html/body/div/div[13]/div[2]/div/div[4]/form/div[2]/div/button'\n",
    "    year_button_path = '/html/body/div/div[13]/div[2]/div/div[4]/form/div[1]/table/thead/tr[2]/th[3]/span/a'\n",
    "    driver = webdriver.Chrome()\n",
    "    time.sleep(2)\n",
    "\n",
    "    path = (\"../data/SOC_Researchers_with_doras_names.csv\")\n",
    "\n",
    "    researchers = pd.read_csv(path, ,encoding=\"ISO-8859-1\")\n",
    "\n",
    "    for index, value in researchers['Researcher'].items():\n",
    "        name = value.strip()\n",
    "        google_scholar_link = researchers.iloc[index, 1]\n",
    "        if str(google_scholar_link) != 'nan':\n",
    "            try:\n",
    "                driver.get(google_scholar_link)\n",
    "            except TimeoutException:\n",
    "                try:\n",
    "                    driver.get(google_scholar_link)\n",
    "                except TimeoutException:\n",
    "                    driver.get(google_scholar_link)\n",
    "            time.sleep(5 + random.uniform(0, 20))\n",
    "            #maxing out the show more button\n",
    "            show_more_button = driver.find_element_by_xpath(show_more_button_path)\n",
    "            time.sleep(2)\n",
    "            while show_more_button.get_attribute('disabled') == None:\n",
    "                WebDriverWait(driver, 20).until(EC.element_to_be_clickable(\n",
    "                    (By.XPATH, show_more_button_path))).click()\n",
    "                time.sleep(3 + random.uniform(0, 1))\n",
    "                show_more_button = driver.find_element_by_xpath(show_more_button_path)\n",
    "            WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,year_button_path))).click()\n",
    "            time.sleep(2)\n",
    "            text = driver.page_source\n",
    "            list_researcher_name = []\n",
    "            list_paper_title = []\n",
    "            list_year = []\n",
    "            list_authors = []\n",
    "            list_journal_info = []\n",
    "            list_count_cit = []\n",
    "            list_only_journal = []\n",
    "            soup = BeautifulSoup(text, \"lxml\")\n",
    "            for instance in soup.find_all('tr', class_='gsc_a_tr'):\n",
    "\n",
    "                #getting year\n",
    "                year = instance.find(\"span\", class_=\"gsc_a_h gsc_a_hc gs_ibl\")\n",
    "                if (int(year.get_text().strip())) < start_year or (int(year.get_text().strip())) > end_year:\n",
    "                    break\n",
    "                if len(year) > 0:\n",
    "                    list_year.append(int(year.get_text().strip()))\n",
    "                else:\n",
    "                    list_year.append(None)\n",
    "                #adding research name\n",
    "                list_researcher_name.append(name)\n",
    "\n",
    "                # finding authors names\n",
    "\n",
    "                authors = instance.find('div', class_='gs_gray')\n",
    "                #authors_text = authors.get_text().replace(\"â€¦\", \"\")\n",
    "                list_authors.append(\n",
    "                    authors.get_text().replace(u'\\xa0', u' ').strip())\n",
    "\n",
    "                #finding the papers title\n",
    "                paper_title = instance.find(\"a\", class_=\"gsc_a_at\")\n",
    "                list_paper_title.append(\n",
    "                    paper_title.get_text().replace(u'\\xa0', u' ').strip())\n",
    "\n",
    "                #finding the journal name\n",
    "                journal_info = instance.find_all('div', class_='gs_gray')[1]\n",
    "                if journal_info:\n",
    "                    list_journal_info.append(\n",
    "                        journal_info.get_text().replace(u'\\xa0', u' ').strip())\n",
    "                    if year:\n",
    "                        list_only_journal.append(\"\".join(journal_info.get_text().replace(\n",
    "                            u'\\xa0', u' ').strip().split(\",\")[:-1]))\n",
    "                    else:\n",
    "                        list_only_journal.append(\n",
    "                            journal_info.get_text().replace(u'\\xa0', u' ').strip())\n",
    "\n",
    "                else:\n",
    "                    list_journal_info.append(None)\n",
    "                    list_only_journal.append(None)\n",
    "\n",
    "                #getting count of citations\n",
    "                count_cit = instance.find('a', class_=\"gsc_a_ac gs_ibl\")\n",
    "                # checking if its not just a line through citation\n",
    "                if count_cit == None:\n",
    "                    count_cit = instance.find(\n",
    "                        'a', class_=\"gsc_a_ac gs_ibl gsc_a_acm\")\n",
    "                if len(count_cit) > 0:\n",
    "                    list_count_cit.append(count_cit.get_text().strip())\n",
    "                else:\n",
    "                    list_count_cit.append(0)\n",
    "            d = {\"Research name\": list_researcher_name, \"Publication Title\": list_paper_title, \"Author List\": list_authors,\n",
    "                 \"Conf/Journal Details\": list_only_journal, \"Citation count\": list_count_cit, \"Year\": list_year}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            pd.to_numeric(df.Year, errors='coerce')\n",
    "            df = df.sort_values(by=['Year'], ascending=False)\n",
    "            filename = \"_\".join(name.split(\" \"))\n",
    "            path_2 = \"../data/Google Scholar Publications/{}.csv\".format(filename)\n",
    "            df.to_csv(path_2, index=None, header=True)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_doras_researcher(start_year=2009,end_year=datetime.datetime.now().year):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(30)\n",
    "    driver.set_page_load_timeout(30)\n",
    "\n",
    "    time.sleep(2)\n",
    "    researchers = pd.read_csv(\"data/SOC_Researchers_with_doras_names.csv\",encoding=\"ISO-8859-1\")\n",
    "\n",
    "\n",
    "    def remove_orcid_number(list_info):\n",
    "        i = 0\n",
    "        removed = []\n",
    "        while i < len(list_info):\n",
    "            if list_info[i] == \"ORCID:\":\n",
    "                i = i + 2\n",
    "            removed.append(list_info[i])\n",
    "\n",
    "            i += 1\n",
    "        return removed\n",
    "\n",
    "\n",
    "    def get_names(text):\n",
    "        list_names = []\n",
    "        for name in text:\n",
    "            name = name.strip()\n",
    "            if \"(\" in name:\n",
    "                break\n",
    "            if not name.isspace() and name != '' and name != \"\\n\" and name != \",\" and name != \"and\" and \"ORCID:\" not in name:\n",
    "                list_names.append(name)\n",
    "        return list_names\n",
    "\n",
    "\n",
    "    def get_orcid(text):\n",
    "        list_orcid_names = []\n",
    "        list_just_name = []\n",
    "        list_names = []\n",
    "        for name in text:\n",
    "            name = name.strip()\n",
    "            if \"(\" in name:\n",
    "                break\n",
    "            if not name.isspace() and name != '' and name != \"\\n\" and name != \",\" and name != \"and\":\n",
    "                list_names.append(name)\n",
    "        i = 0\n",
    "        while i < len(list_names):\n",
    "            if \"ORCID:\" in list_names[i]:\n",
    "                list_orcid_names.append(list_names[i - 1] + \", \" + list_names[i])\n",
    "            if i != len(list_names) - 1:\n",
    "                if \"ORCID:\" not in list_names[i] and \"ORCID:\" not in list_names[i + 1]:\n",
    "                    list_just_name.append(list_names[i])\n",
    "            else:\n",
    "                if \"ORCID:\" not in list_names[i]:\n",
    "                    list_just_name.append(list_names[i])\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        return list_orcid_names, list_just_name\n",
    "\n",
    "\n",
    "    def get_isbn(text):\n",
    "        isbn = text.split(\".\")[-1]\n",
    "        number = None\n",
    "        if \"ISBN\" in isbn:\n",
    "            number = isbn.split(\" \")[-1]\n",
    "        return number\n",
    "\n",
    "\n",
    "    def get_issn(text):\n",
    "        issn = text.split(\".\")[-1]\n",
    "        number = None\n",
    "        if \"ISSN\" in issn:\n",
    "            number = issn.split(\" \")[-1]\n",
    "        return number\n",
    "\n",
    "\n",
    "    def remove_issn_isbn(text):\n",
    "        check = text.split(\".\")[-1]\n",
    "        want = text\n",
    "        if \"ISBN\" in check or \"ISSN\" in check:\n",
    "            want = \".\".join(text.split(\".\")[:-1])\n",
    "        return want\n",
    "\n",
    "\n",
    "    for index, value in researchers['Researcher'].items():\n",
    "        list_years = []  # list of years\n",
    "        # authors formated similar to the way it is presented on the doras website.\n",
    "        list_authors = []\n",
    "        # authors name similar to the ones in google scholar format ie first letter of first name and full surname.\n",
    "        list_google_scholar_format = []\n",
    "        list_paper_title = []  # list of paper titles\n",
    "        list_journal_info = []  # list containg information about the publication\n",
    "        # authors name formated as full first name and full surname.\n",
    "        list_correct_name = []\n",
    "        list_orcid = []  # author name and orcid\n",
    "        list_without_orcid = []  # authors that do not have a orid name\n",
    "        list_researcher = []  # just contains the researcher name\n",
    "        list_isbn = []  # list of isbn number\n",
    "        list_just_journal = []  # just the journal\n",
    "        list_isbn = []  # list of isbn numbers\n",
    "        list_issn = []  # list of issn number\n",
    "        list_item_type = []  # list of item types\n",
    "        list_event_type = []  # list of event types\n",
    "        list_refereed = []  # list of references\n",
    "        list_subjects = []  # list of subjects associated with each populication\n",
    "        list_uncontrolled_Keywords = []  # list of uncontrolled keywords\n",
    "        lsit_subjects = []  # list of subjects\n",
    "        list_dcu_faculties_and_centres = []  # list of dcu centres involved\n",
    "        list_published_in = []  # conf/journal details\n",
    "        list_publisher = []  # publisher\n",
    "        list_official_url = []  # url of each publication\n",
    "        list_copyright_information = []  # copyright info\n",
    "        list_use_license = []  # use license details\n",
    "        list_date_of_award = []  # list of the date a thesis was awarded\n",
    "        list_supervisors = []  # list of supervisors associated with a thesis\n",
    "        list_funders = []  # list of funders\n",
    "        list_id_code = []  # list of the unique doras id code associated with each publication\n",
    "        list_additional_information = []  # additional info\n",
    "        # list of  details such as the date and the person who inputed the details of the publication into the doras system\n",
    "        list_deposited_on = []\n",
    "        list_tweet = []  # list of the number of the tweets garned by each publicatiom\n",
    "        list_mendeley = []  # list of the number of meneley readers garned by each publicatiom\n",
    "\n",
    "        researcher = value.strip()\n",
    "        doras_link = researchers.iloc[index, 2]\n",
    "        if str(doras_link) != 'nan':\n",
    "            res = requests.get(doras_link)\n",
    "            soup = BeautifulSoup(res.text, \"lxml\")\n",
    "            # looping through every header object to get year\n",
    "            for header in soup.find_all('h2'):\n",
    "                nextNode = header\n",
    "                year = nextNode.get_text()\n",
    "                while True:\n",
    "                    nextNode = nextNode.nextSibling\n",
    "                    # various conditions that if met the node shoud move on to the next header tag\n",
    "                    if nextNode is None:\n",
    "                        break\n",
    "                    if isinstance(nextNode, NavigableString):\n",
    "                        print(nextNode.strip())\n",
    "                    if isinstance(nextNode, Tag):\n",
    "                        if nextNode.name == \"h2\":\n",
    "                            break\n",
    "                        elif nextNode.name == \"a\":\n",
    "                            break\n",
    "                        elif nextNode.attrs == {'class': ['ep_view_timestamp']}:\n",
    "                            break\n",
    "                    if int(year) < int(start_year) or int(year) > int(end_year):\n",
    "                        break\n",
    "                    names = get_names(nextNode.find_all(text=True,))\n",
    "                    # storing all of the names of that paper so we can perform string manipulation\n",
    "                    names_of_paper = []\n",
    "                    google_scholar_format = []\n",
    "                    correct_name = []\n",
    "                    list_researcher.append(researcher)\n",
    "                    with_orcid, no_orcid_name = get_orcid(\n",
    "                        nextNode.find_all(text=True,))\n",
    "                    if len(\", \".join(with_orcid).strip()) > 0:\n",
    "                        list_orcid.append(\", \".join(with_orcid).strip())\n",
    "                    else:\n",
    "                        list_orcid.append(None)\n",
    "\n",
    "                    journal_info = None\n",
    "                    i = 0\n",
    "                    while i < len(names):\n",
    "                        # taking  author list and making sure it doesnt have an extra comma at end and removing the and\n",
    "                        if i != len(names) - 1:\n",
    "                            second = names[i].split(\",\")[0]\n",
    "                            first = names[i].split(\",\")[-1]\n",
    "                            names_of_paper.append(\n",
    "                                second.strip() + \", \" + first.strip() + \", \")\n",
    "                            correct_name.append(\n",
    "                                first.strip() + \" \" + second.strip() + \", \")\n",
    "                        else:\n",
    "                            second = names[i].split(\",\")[0]\n",
    "                            first = names[i].split(\",\")[-1]\n",
    "                            names_of_paper.append(\n",
    "                                second.strip() + \", \" + first.strip())\n",
    "                            correct_name.append(\n",
    "                                first.strip() + \" \" + second.strip())\n",
    "                        i = i + 1\n",
    "\n",
    "                    #turniing the list of authors into a string representation\n",
    "                    names_of_paper = \"\".join(names_of_paper).strip()\n",
    "\n",
    "                    correct_name = \"\".join(correct_name)\n",
    "\n",
    "                    #adding the names to the revalent lists\n",
    "                    list_authors.append(names_of_paper)\n",
    "                    list_google_scholar_format.append(google_scholar_format)\n",
    "                    list_correct_name.append(correct_name)\n",
    "\n",
    "                    #adding the correct year\n",
    "                    list_years.append(int(year))\n",
    "\n",
    "                    #authors that do not have a orcid\n",
    "                    if len(\", \".join(no_orcid_name).strip()) > 0:\n",
    "                        list_without_orcid.append(\", \".join(no_orcid_name).strip())\n",
    "                    else:\n",
    "                        list_without_orcid.append(None)\n",
    "\n",
    "                    #adding extra information to each publication that is detailed in the link attached to each publication title\n",
    "                    extra_link = None\n",
    "                    if len(nextNode.find_all('a')) > 1:\n",
    "                        for link in nextNode.find_all('a'):\n",
    "                            if \"doras\" in str(link[\"href\"]):\n",
    "                                extra_link = link[\"href\"]\n",
    "                    else:\n",
    "                        extra_link = nextNode.find(\"a\")[\"href\"]\n",
    "                    num_tweet = None\n",
    "                    num_mend = None\n",
    "                    item_type = None\n",
    "                    event_type = None\n",
    "                    refereed = None\n",
    "                    uncontrolled_Keywords = None\n",
    "                    subjects = None\n",
    "                    dcu_faculties_and_centres = None\n",
    "                    published_in = None\n",
    "                    publisher = None\n",
    "                    official_url = None\n",
    "                    copyright_information = None\n",
    "                    use_license = None\n",
    "                    date_of_award = None\n",
    "                    supervisors = None\n",
    "                    funders = None\n",
    "                    id_code = None\n",
    "                    additional_information = None\n",
    "                    deposited_on = None\n",
    "                    try:\n",
    "                        driver.get(extra_link)\n",
    "                    except TimeoutException:\n",
    "                        try:\n",
    "                            time.sleep(2)\n",
    "                            driver.get(extra_link)\n",
    "                        except TimeoutException:\n",
    "                            time.sleep(2)\n",
    "                            driver.get(extra_link)\n",
    "\n",
    "                    time.sleep(2 + random.uniform(0, 6))\n",
    "                    soupes = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                    try:\n",
    "                        num_tweet = soupes.find(\n",
    "                            \"div\", class_=\"altmetric_row altmetric_tweeters\").get_text()\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "                    try:\n",
    "                        num_mend = soupes.find(\n",
    "                            \"div\", class_=\"altmetric_row altmetric_mendeley\").get_text()\n",
    "                    except AttributeError:\n",
    "                        pass\n",
    "                    table = soupes.find(\n",
    "                        'table', class_=\"ep_block\", style=\"margin-bottom: 1em; text-align:left;\", border=\"0\")\n",
    "                    for row in table.find_all(\"tr\"):\n",
    "                        descript = row.find(\n",
    "                            \"th\", class_=\"ep_row\").get_text().strip()\n",
    "                        value = row.find('td', class_=\"ep_row\").get_text().strip()\n",
    "\n",
    "                        if descript == 'Item Type:':\n",
    "                            item_type = value\n",
    "\n",
    "                        if descript == \"Event Type:\":\n",
    "                            event_type = value\n",
    "\n",
    "                        if descript == 'Date of Award:':\n",
    "                            date_of_award = value\n",
    "                        if descript == \"Refereed:\":\n",
    "                            refereed = value\n",
    "                        if descript == \"Supervisor(s):\":\n",
    "                            supervisors = value\n",
    "                        if descript == \"Uncontrolled Keywords:\":\n",
    "                            uncontrolled_Keywords = value\n",
    "\n",
    "                        if descript == \"Subjects:\":\n",
    "                            if len(row.find_all(\"a\")) <= 1:\n",
    "                                subjects = value\n",
    "                            else:\n",
    "                                subjects = \"\"\n",
    "                                for text in row.find_all(\"a\"):\n",
    "                                    subjects += text.get_text().strip() + \". \"\n",
    "\n",
    "                        if descript == 'DCU Faculties and Centres:':\n",
    "                            if len(row.find_all(\"a\")) <= 1:\n",
    "                                dcu_faculties_and_centres = value\n",
    "                            else:\n",
    "                                dcu_faculties_and_centres = \"\"\n",
    "                                for text in row.find_all(\"a\"):\n",
    "                                    dcu_faculties_and_centres += text.get_text().strip() + \". \"\n",
    "\n",
    "                        if descript == \"Use License:\":\n",
    "                            use_license = value\n",
    "\n",
    "                        if descript == \"ID Code:\":\n",
    "                            id_code = value\n",
    "\n",
    "                        if descript == \"Deposited On:\":\n",
    "                            deposited_on = \" \".join([item.replace(\"\\n\", \"\").strip(\n",
    "                            ) for item in value.split(\" \") if item != \"\" or item != ''])\n",
    "\n",
    "                        if descript == \"Published in:\":\n",
    "                            published_in = \" \".join([item.replace(\"\\n\", \"\").strip(\n",
    "                            ) for item in value.split(\" \") if item != \"\" or item != ''])\n",
    "\n",
    "                        if descript == \"Publisher:\":\n",
    "                            publisher = value\n",
    "                        if descript == \"Official URL:\":\n",
    "                            official_url = value\n",
    "\n",
    "                        if descript == \"Copyright Information:\":\n",
    "                            copyright_information = value\n",
    "\n",
    "                        if descript == \"Funders:\":\n",
    "                            funders = value\n",
    "\n",
    "                        if descript == \"Additional Information:\":\n",
    "                            additional_information = value\n",
    "\n",
    "                    list_item_type.append(item_type)\n",
    "                    list_event_type.append(event_type)\n",
    "                    list_refereed.append(refereed)\n",
    "                    list_uncontrolled_Keywords.append(uncontrolled_Keywords)\n",
    "                    lsit_subjects.append(subjects)\n",
    "                    list_dcu_faculties_and_centres.append(\n",
    "                        dcu_faculties_and_centres)\n",
    "                    list_published_in.append(published_in)\n",
    "                    list_publisher.append(publisher)\n",
    "                    list_official_url.append(official_url)\n",
    "                    list_copyright_information.append(copyright_information)\n",
    "                    list_use_license.append(use_license)\n",
    "                    list_date_of_award.append(date_of_award)\n",
    "                    list_supervisors.append(supervisors)\n",
    "                    list_funders.append(funders)\n",
    "                    list_id_code.append(id_code)\n",
    "                    list_additional_information.append(additional_information)\n",
    "                    list_deposited_on.append(deposited_on)\n",
    "                    list_subjects.append(subjects)\n",
    "                    list_tweet.append(num_tweet)\n",
    "                    list_mendeley.append(num_mend)\n",
    "\n",
    "                    # getting the title of the paper\n",
    "                    paper_title = nextNode.find(\"em\")\n",
    "                    #adding paper title to a list of paper titles\n",
    "                    list_paper_title.append(paper_title.get_text().strip())\n",
    "\n",
    "                    #info and information represent the publication's information at different cleaning stages.\n",
    "                    info = str(nextNode.find('em').next.next.strip())\n",
    "                    information = [inc.strip() for inc in info.split(\n",
    "                        \" \") if not inc.isspace() and inc != '']\n",
    "\n",
    "                    #information is a list of journal info\n",
    "                    #checking that it is not a book\n",
    "                    if len(information) > 0:\n",
    "                        if information[0] == 'In:':\n",
    "                            information = information[1:]  # removing \"In:\"\n",
    "                            # details about the journal inluding details such as date and ISBN\n",
    "                            journal_info = \" \".join(information)\n",
    "                        elif information[0] == 'PhD':  # PhD's treated differently\n",
    "                            journal_info = \" \".join(information)\n",
    "                        else:\n",
    "                            journal_info = \" \".join(information)\n",
    "                    elif len(information) == 0:  # testing if its a book rather than a journal\n",
    "                        # lines below are my cleaning process for book's\n",
    "                        lines = nextNode.find_all(text=True,)\n",
    "                        i = 0\n",
    "                        position = 0\n",
    "                        wanted = []\n",
    "                        while i < len(lines):\n",
    "                            line = lines[i].strip()\n",
    "                            wanted.append(lines[i])\n",
    "                            if line == 'In: <if test=\"!is_set(creators)\"><print expr=\"editors_name\"/>, (ed<if test=\"length(editors_name) gt 1\">s</if>.)</if>':\n",
    "                                position = i + 1\n",
    "                            i += 1\n",
    "                        wanted = \"\".join(wanted[position:])\n",
    "                        position = 0\n",
    "                        info_want = []\n",
    "                        for want in wanted.split(\" \"):\n",
    "                            if not want.isspace() and want != '' and want != \"\\n\":\n",
    "                                    info_want.append(want.strip())\n",
    "                        info_want = remove_orcid_number(info_want)\n",
    "                        info_want = info_want[1:]  # removing \"In:\"\n",
    "                        journal_info = \" \".join(info_want)\n",
    "\n",
    "                    # adding journal info to a list of journal information\n",
    "                    list_journal_info.append(\n",
    "                        remove_issn_isbn(journal_info.strip()))\n",
    "\n",
    "                    isbn_num = get_isbn(journal_info.strip())\n",
    "                    list_isbn.append(isbn_num)\n",
    "                    issn_num = get_issn(journal_info.strip())\n",
    "                    list_issn.append(issn_num)\n",
    "\n",
    "            d = {\"Research name\": list_researcher, \"Publication Title\": list_paper_title,  \"Author List\": list_authors, \"Conf/Journal Details\": list_journal_info,  \"Year\": list_years, \"Full name\": list_correct_name, \"Authors and Orcid\": list_orcid, \"Authors without a orcid\": list_without_orcid, \"ISBN\": list_isbn, \"ISSN\": list_issn, 'Item Type': list_item_type, \"Event Type\": list_event_type, \"Refereed\": list_refereed, 'Date of Award': list_date_of_award,\n",
    "                \"Supervisor(s)\": list_supervisors, \"Uncontrolled Keywords\": list_uncontrolled_Keywords, \"Subject\": list_subjects, 'DCU Faculties and Centres': list_dcu_faculties_and_centres, \"Use License\": list_use_license, \"ID Code\": list_id_code, \"Deposited On\": list_deposited_on, \"Published in\": list_published_in, \"Publisher\": list_publisher, \"Official URL\": list_official_url, \"Copyright Information\": list_copyright_information, \"Funders\": list_funders, \"Additional Information\": list_additional_information, \"Tweets\": list_tweet, \"Mendeley Readers\": list_mendeley}\n",
    "            df = pd.DataFrame(data=d)\n",
    "            pd.to_numeric(df.Year , errors='coerce')\n",
    "            df[\"Publication Title\"] = df[\"Publication Title\"].apply(lambda x: \" \".join(str(x).splitlines())) \n",
    "            filename = \"_\".join(researcher.split(\" \"))\n",
    "            path = \"../data/Doras publications/{}.csv\".format(filename)\n",
    "            df.to_csv(path, index=None, header=True)\n",
    "\n",
    "\n",
    "    driver.quit()\n",
    "    \n",
    "\n",
    "def get_doras_dcu(start_year=2009,end_year=datetime.datetime.now().year):\n",
    "    res = requests.get(\"http://doras.dcu.ie/view/groups/groups2a.html\")\n",
    "\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.implicitly_wait(30)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    time.sleep(2)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    list_years = []  # list of years\n",
    "\n",
    "\n",
    "    # authors formated similar to the way it is presented on the doras website.\n",
    "    list_authors = []\n",
    "    # authors name similar to the ones in google scholar format ie first letter of first name and full surname.\n",
    "    list_google_scholar_format = []\n",
    "    list_paper_title = []  # list of paper titles\n",
    "    list_journal_info = []  # list containg information about the publication\n",
    "    # authors name formated as full first name and full surname.\n",
    "    list_correct_name = []\n",
    "    list_orcid = []  # author name and orcid\n",
    "    list_without_orcid = []  # authors that do not have a orid\n",
    "    list_isbn = []  # list of isbn numbers\n",
    "    list_issn = []  # list of issn numbers\n",
    "\n",
    "    list_item_type = []  # list of item types\n",
    "    list_event_type = []  # list of event types\n",
    "    list_refereed = []  # list of references\n",
    "    list_subjects = []  # list of subjects associated with each populication\n",
    "    list_uncontrolled_Keywords = []  # list of uncontrolled keywords\n",
    "    lsit_subjects = []  # list of subjects\n",
    "    list_dcu_faculties_and_centres = []  # list of dcu centres involved\n",
    "    list_published_in = []  # conf/journal details\n",
    "    list_publisher = []  # publisher\n",
    "    list_official_url = []  # url of each publication\n",
    "    list_copyright_information = []  # copyright info\n",
    "    list_use_license = []  # use license details\n",
    "    list_date_of_award = []  # list of the date a thesis was awarded\n",
    "    list_supervisors = []  # list of supervisors associated with a thesis\n",
    "    list_funders = []  # list of funders\n",
    "    list_id_code = []  # list of the unique doras id code associated with each publication\n",
    "    list_additional_information = []  # additional info\n",
    "    # list of  details such as the date and the person who inputed the details of the publication into the doras system\n",
    "    list_deposited_on = []\n",
    "    list_tweet = []  # list of the number of the tweets garned by each publicatiom\n",
    "    list_mendeley = []  # list of the number of meneley readers garned by each publicatiom\n",
    "\n",
    "\n",
    "    def remove_orcid_number(list_info):\n",
    "        i = 0\n",
    "        removed = []\n",
    "        while i < len(list_info):\n",
    "            if list_info[i] == \"ORCID:\":\n",
    "                i = i + 2\n",
    "            removed.append(list_info[i])\n",
    "\n",
    "            i += 1\n",
    "        return removed\n",
    "\n",
    "\n",
    "    def get_names(text):\n",
    "        list_names = []\n",
    "        for name in text:\n",
    "            name = name.strip()\n",
    "            if \"(\" in name:\n",
    "                break\n",
    "            if not name.isspace() and name != '' and name != \"\\n\" and name != \",\" and name != \"and\" and \"ORCID:\" not in name:\n",
    "                list_names.append(name)\n",
    "        return list_names\n",
    "\n",
    "\n",
    "    def get_orcid(text):\n",
    "        list_orcid_names = []\n",
    "        list_just_name = []\n",
    "        list_names = []\n",
    "        for name in text:\n",
    "            name = name.strip()\n",
    "            if \"(\" in name:\n",
    "                break\n",
    "            if not name.isspace() and name != '' and name != \"\\n\" and name != \",\" and name != \"and\":\n",
    "                list_names.append(name)\n",
    "        i = 0\n",
    "        while i < len(list_names):\n",
    "            if \"ORCID:\" in list_names[i]:\n",
    "                list_orcid_names.append(list_names[i - 1] + \", \" + list_names[i])\n",
    "            if i != len(list_names) - 1:\n",
    "                if \"ORCID:\" not in list_names[i] and \"ORCID:\" not in list_names[i + 1]:\n",
    "                    list_just_name.append(list_names[i])\n",
    "            else:\n",
    "                if \"ORCID:\" not in list_names[i]:\n",
    "                    list_just_name.append(list_names[i])\n",
    "\n",
    "            i = i + 1\n",
    "\n",
    "        return list_orcid_names, list_just_name\n",
    "\n",
    "\n",
    "    def get_isbn(text):\n",
    "        isbn = text.split(\".\")[-1]\n",
    "        number = None\n",
    "        if \"ISBN\" in isbn:\n",
    "            number = isbn.split(\" \")[-1]\n",
    "        return number\n",
    "\n",
    "\n",
    "    def get_issn(text):\n",
    "        issn = text.split(\".\")[-1]\n",
    "        number = None\n",
    "        if \"ISSN\" in issn:\n",
    "            number = issn.split(\" \")[-1]\n",
    "        return number\n",
    "\n",
    "\n",
    "    def remove_issn_isbn(text):\n",
    "        check = text.split(\".\")[-1]\n",
    "        want = text\n",
    "        if \"ISBN\" in check or \"ISSN\" in check:\n",
    "            want = \".\".join(text.split(\".\")[:-1])\n",
    "        return want\n",
    "\n",
    "    # looping through every header object to get year\n",
    "    for header in soup.find_all('h2'):\n",
    "        nextNode = header\n",
    "        year = nextNode.get_text()\n",
    "        while True:\n",
    "            nextNode = nextNode.nextSibling\n",
    "            # various conditions that if met the node shoud move on to the next header tag\n",
    "            if nextNode is None:\n",
    "                break\n",
    "            if isinstance(nextNode, NavigableString):\n",
    "                print(nextNode.strip())\n",
    "            if isinstance(nextNode, Tag):\n",
    "                if nextNode.name == \"h2\":\n",
    "                    break\n",
    "                elif nextNode.name == \"a\":\n",
    "                    break\n",
    "                elif nextNode.attrs == {'class': ['ep_view_timestamp']}:\n",
    "                    break\n",
    "            if int(year) < int(start_year) or int(year) > int(end_year):\n",
    "                break\n",
    "            names = get_names(nextNode.find_all(text=True,))\n",
    "            # storing all of the names of that paper so we can perform string manipulation\n",
    "            names_of_paper = []\n",
    "            google_scholar_format = []\n",
    "            correct_name = []\n",
    "            with_orcid, no_orcid_name = get_orcid(nextNode.find_all(text=True,))\n",
    "            if len(\", \".join(with_orcid).strip()) > 0:\n",
    "                list_orcid.append(\", \".join(with_orcid).strip())\n",
    "            else:\n",
    "                list_orcid.append(None)\n",
    "\n",
    "            journal_info = None\n",
    "            i = 0\n",
    "            while i < len(names):\n",
    "                # taking  author list and making sure it doesnt have an extra comma at end and removing the and\n",
    "                if i != len(names) - 1:\n",
    "                    second = names[i].split(\",\")[0]\n",
    "                    first = names[i].split(\",\")[-1]\n",
    "                    names_of_paper.append(\n",
    "                        second.strip() + \", \" + first.strip() + \", \")\n",
    "                    correct_name.append(first.strip() + \" \" +\n",
    "                                        second.strip() + \", \")\n",
    "                else:\n",
    "                    second = names[i].split(\",\")[0]\n",
    "                    first = names[i].split(\",\")[-1]\n",
    "                    names_of_paper.append(second.strip() + \", \" + first.strip())\n",
    "                    correct_name.append(first.strip() + \" \" + second.strip())\n",
    "                i = i + 1\n",
    "\n",
    "            #turniing the list of authors into a string representation\n",
    "            names_of_paper = \"\".join(names_of_paper).strip()\n",
    "\n",
    "            correct_name = \"\".join(correct_name)\n",
    "\n",
    "            #adding the names to the revalent lists\n",
    "            list_authors.append(names_of_paper)\n",
    "            list_google_scholar_format.append(google_scholar_format)\n",
    "            list_correct_name.append(correct_name)\n",
    "\n",
    "            #adding the correct year\n",
    "            list_years.append(int(year))\n",
    "\n",
    "            #authors that do not have a orcid\n",
    "            if len(\", \".join(no_orcid_name).strip()) > 0:\n",
    "                list_without_orcid.append(\", \".join(no_orcid_name).strip())\n",
    "            else:\n",
    "                list_without_orcid.append(None)\n",
    "\n",
    "            #adding extra information to each publication that is detailed in the link attached to each publication title\n",
    "            extra_link = None\n",
    "            if len(nextNode.find_all('a')) > 1:\n",
    "                for link in nextNode.find_all('a'):\n",
    "                    if \"doras\" in str(link[\"href\"]):\n",
    "                        extra_link = link[\"href\"]\n",
    "            else:\n",
    "                extra_link = nextNode.find(\"a\")[\"href\"]\n",
    "            num_tweet = None\n",
    "            num_mend = None\n",
    "            item_type = None\n",
    "            event_type = None\n",
    "            refereed = None\n",
    "            uncontrolled_Keywords = None\n",
    "            subjects = None\n",
    "            dcu_faculties_and_centres = None\n",
    "            published_in = None\n",
    "            publisher = None\n",
    "            official_url = None\n",
    "            copyright_information = None\n",
    "            use_license = None\n",
    "            date_of_award = None\n",
    "            supervisors = None\n",
    "            funders = None\n",
    "            id_code = None\n",
    "            additional_information = None\n",
    "            deposited_on = None\n",
    "            try:\n",
    "                driver.get(extra_link)\n",
    "            except TimeoutException:\n",
    "                try:\n",
    "                    time.sleep(2)\n",
    "                    driver.get(extra_link)\n",
    "                except TimeoutException:\n",
    "                    time.sleep(2)\n",
    "                    driver.get(extra_link)\n",
    "\n",
    "            time.sleep(2)\n",
    "            soupes = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                num_tweet = soupes.find(\n",
    "                    \"div\", class_=\"altmetric_row altmetric_tweeters\").get_text()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            try:\n",
    "                num_mend = soupes.find(\n",
    "                    \"div\", class_=\"altmetric_row altmetric_mendeley\").get_text()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            table = soupes.find('table', class_=\"ep_block\",\n",
    "                                style=\"margin-bottom: 1em; text-align:left;\", border=\"0\")\n",
    "            for row in table.find_all(\"tr\"):\n",
    "                descript = row.find(\"th\", class_=\"ep_row\").get_text().strip()\n",
    "                value = row.find('td', class_=\"ep_row\").get_text().strip()\n",
    "\n",
    "                if descript == 'Item Type:':\n",
    "                    item_type = value\n",
    "\n",
    "                if descript == \"Event Type:\":\n",
    "                    event_type = value\n",
    "\n",
    "                if descript == 'Date of Award:':\n",
    "                    date_of_award = value\n",
    "                if descript == \"Refereed:\":\n",
    "                    refereed = value\n",
    "                if descript == \"Supervisor(s):\":\n",
    "                    supervisors = value\n",
    "                if descript == \"Uncontrolled Keywords:\":\n",
    "                    uncontrolled_Keywords = value\n",
    "\n",
    "                if descript == \"Subjects:\":\n",
    "                    if len(row.find_all(\"a\")) <= 1:\n",
    "                        subjects = value\n",
    "                    else:\n",
    "                        subjects = \"\"\n",
    "                        for text in row.find_all(\"a\"):\n",
    "                            subjects += text.get_text().strip() + \". \"\n",
    "\n",
    "                if descript == 'DCU Faculties and Centres:':\n",
    "                    if len(row.find_all(\"a\")) <= 1:\n",
    "                        dcu_faculties_and_centres = value\n",
    "                    else:\n",
    "                        dcu_faculties_and_centres = \"\"\n",
    "                        for text in row.find_all(\"a\"):\n",
    "                            dcu_faculties_and_centres += text.get_text().strip() + \". \"\n",
    "\n",
    "                if descript == \"Use License:\":\n",
    "                    use_license = value\n",
    "\n",
    "                if descript == \"ID Code:\":\n",
    "                    id_code = value\n",
    "\n",
    "                if descript == \"Deposited On:\":\n",
    "                    deposited_on = \" \".join([item.replace(\"\\n\", \"\").strip(\n",
    "                    ) for item in value.split(\" \") if item != \"\" or item != ''])\n",
    "\n",
    "                if descript == \"Published in:\":\n",
    "                    published_in = \" \".join([item.replace(\"\\n\", \"\").strip(\n",
    "                    ) for item in value.split(\" \") if item != \"\" or item != ''])\n",
    "\n",
    "                if descript == \"Publisher:\":\n",
    "                    publisher = value\n",
    "                if descript == \"Official URL:\":\n",
    "                    official_url = value\n",
    "\n",
    "                if descript == \"Copyright Information:\":\n",
    "                    copyright_information = value\n",
    "\n",
    "                if descript == \"Funders:\":\n",
    "                    funders = value\n",
    "\n",
    "                if descript == \"Additional Information:\":\n",
    "                    additional_information = value\n",
    "\n",
    "            list_item_type.append(item_type)\n",
    "            list_event_type.append(event_type)\n",
    "            list_refereed.append(refereed)\n",
    "            list_uncontrolled_Keywords.append(uncontrolled_Keywords)\n",
    "            lsit_subjects.append(subjects)\n",
    "            list_dcu_faculties_and_centres.append(dcu_faculties_and_centres)\n",
    "            list_published_in.append(published_in)\n",
    "            list_publisher.append(publisher)\n",
    "            list_official_url.append(official_url)\n",
    "            list_copyright_information.append(copyright_information)\n",
    "            list_use_license.append(use_license)\n",
    "            list_date_of_award.append(date_of_award)\n",
    "            list_supervisors.append(supervisors)\n",
    "            list_funders.append(funders)\n",
    "            list_id_code.append(id_code)\n",
    "            list_additional_information.append(additional_information)\n",
    "            list_deposited_on.append(deposited_on)\n",
    "            list_subjects.append(subjects)\n",
    "            list_tweet.append(num_tweet)\n",
    "            list_mendeley.append(num_mend)\n",
    "\n",
    "            # getting the title of the paper\n",
    "            paper_title = nextNode.find(\"em\")\n",
    "            #adding paper title to a list of paper titles\n",
    "            String_title = None\n",
    "            string_title = re.sub('\\r\\n', ' ', paper_title.get_text().strip())\n",
    "            list_paper_title.append(string_title)\n",
    "\n",
    "            #info and information represent the publication's information at different cleaning stages.\n",
    "            info = str(nextNode.find('em').next.next.strip())\n",
    "            information = [inc.strip() for inc in info.split(\n",
    "                \" \") if not inc.isspace() and inc != '']\n",
    "\n",
    "            #information is a list of journal info\n",
    "            #checking that it is not a book\n",
    "            if len(information) > 0:\n",
    "                if information[0] == 'In:':\n",
    "                    information = information[1:]  # removing \"In:\"\n",
    "                    # details about the journal inluding details such as date and ISBN\n",
    "                    journal_info = \" \".join(information)\n",
    "                elif information[0] == 'PhD':  # PhD's treated differently\n",
    "                    journal_info = \" \".join(information)\n",
    "                else:\n",
    "                    journal_info = \" \".join(information)\n",
    "            elif len(information) == 0:  # testing if its a book rather than a journal\n",
    "                # lines below are my cleaning process for book's\n",
    "                lines = nextNode.find_all(text=True,)\n",
    "                i = 0\n",
    "                position = 0\n",
    "                wanted = []\n",
    "                while i < len(lines):\n",
    "                    line = lines[i].strip()\n",
    "                    wanted.append(lines[i])\n",
    "                    if line == 'In: <if test=\"!is_set(creators)\"><print expr=\"editors_name\"/>, (ed<if test=\"length(editors_name) gt 1\">s</if>.)</if>':\n",
    "                        position = i + 1\n",
    "                    i += 1\n",
    "                wanted = \"\".join(wanted[position:])\n",
    "                position = 0\n",
    "                info_want = []\n",
    "                for want in wanted.split(\" \"):\n",
    "                    if not want.isspace() and want != '' and want != \"\\n\":\n",
    "                            info_want.append(want.strip())\n",
    "                info_want = remove_orcid_number(info_want)\n",
    "                info_want = info_want[1:]  # removing \"In:\"\n",
    "                journal_info = \" \".join(info_want)\n",
    "\n",
    "            # adding journal info to a list of journal information\n",
    "            list_journal_info.append(remove_issn_isbn(journal_info.strip()))\n",
    "            #adding the isbn and issn numbers\n",
    "            isbn_num = get_isbn(journal_info.strip())\n",
    "            list_isbn.append(isbn_num)\n",
    "            issn_num = get_issn(journal_info.strip())\n",
    "            list_issn.append(issn_num)\n",
    "\n",
    "    driver.quit()\n",
    "    #creating a pandas dataframe\n",
    "    d = {\"Author List\": list_authors, \"Full name\":list_correct_name, \"Authors and Orcid\":list_orcid, \"Authors without a orcid\": list_without_orcid,\"Publication Title\": list_paper_title, \"Conf/Journal Details\": list_journal_info, \"Year\":list_years , \"ISBN\": list_isbn,\"ISSN\": list_issn,'Item Type': list_item_type, \"Event Type\":list_event_type,\"Refereed\":list_refereed,'Date of Award': list_date_of_award,\"Supervisor(s)\":list_supervisors, \"Uncontrolled Keywords\":list_uncontrolled_Keywords, \"Subject\" :list_subjects,'DCU Faculties and Centres': list_dcu_faculties_and_centres,\"Use License\" :list_use_license,\"ID Code\": list_id_code,\"Deposited On\": list_deposited_on, \"Published in\":list_published_in,\"Publisher\":list_publisher,\"Official URL\":list_official_url,\"Copyright Information\":list_copyright_information,\"Funders\":list_funders,\"Additional Information\":list_additional_information,\"Tweets\":list_tweet,\"Mendeley Readers\":list_mendeley}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    pd.to_numeric(df.Year , errors='coerce')\n",
    "    df[\"Publication Title\"] = df[\"Publication Title\"].apply(lambda x: \" \".join(str(x).splitlines()))\n",
    "    path = (\"../data/Doras SOC/doras_soc.csv\")\n",
    "\n",
    "    df.to_csv(path, index = None, header=True)\n",
    "    \n",
    "\n",
    "def not_on_doras():\n",
    "    my_path = os.path.abspath(os.path.dirname(__file__))\n",
    "    path = os.path.join(my_path, \"data/SOC_Researchers_with_doras_names.csv\")\n",
    "    researchers = pd.read_csv(path,encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # a dataframe containing publications that are in a researcher's google scholar profile but not on doras\n",
    "    not_on_doras = pd.DataFrame(columns=['Research name','Publication Title','Author List','Conf/Journal Details','Citation count','Year'])\n",
    "    # datframe containing ambigious publications\n",
    "    mis_matches = pd.DataFrame(columns=['Research name','Mismatch Title','Nearest Title','Score','Year'])\n",
    "\n",
    "    list_mis_match_title = []\n",
    "    list_nearest_title  = []\n",
    "    list_score = []\n",
    "    list_researcher = []\n",
    "    list_year = []\n",
    "    # method that assists in the development of a dataframe that contains  mismatched publications \n",
    "    def add_mis_matches(research, title,near,score,year):\n",
    "        list_researcher.append(research)\n",
    "        list_mis_match_title.append(title)\n",
    "        list_nearest_title.append(near)\n",
    "        list_score.append(score)\n",
    "        list_year.append(year)\n",
    "    #checks if there is a year present in the publication title and the nearest publication title in doras\n",
    "    def check_years(title,near_title):\n",
    "        try:\n",
    "            year1 = re.findall(\"[2][0][0-9]{2}\",title)[0]\n",
    "            year2 = re.findall(\"[2][0][0-9]{2}\",near_title)[0]\n",
    "        except IndexError:\n",
    "            return False\n",
    "        return True\n",
    "    # tests if the year in a publications title and the year in  the nearest publication title in doras are the same\n",
    "    def equal_year(title,near_title):\n",
    "        year1 = re.findall(\"[2][0-9]{3}\",title)[0]\n",
    "        year2 = re.findall(\"[2][0-9]{3}\",near_title)[0]\n",
    "        return year1 == year2\n",
    "    #another method testing whether the year in a publications title containing \"NICIR\" and the year in the nearest publication title in doras are the same \n",
    "    def check_ncir(title,near_title):\n",
    "        year1 = re.findall(\"NTCIR-[0-9]{2}\",title)[0]\n",
    "        year2 = re.findall(\"NTCIR-[0-9]{2}\",near_title)[0]\n",
    "        return year1 == year2\n",
    "    #another method testing whether the year in a publications title containing \"shared task (SR\" and the year in the nearest publication title in doras are the same \n",
    "    def check_sr(title, near_title):\n",
    "        year1 = re.findall(\"(SR[â€™|'][0-9]{2})\",title)[0][-2:]\n",
    "        year2 = re.findall(\"(SR[â€™|'][0-9]{2})\",near_title)[0][-2:]\n",
    "        return year1 == year2\n",
    "    # checks whether a  publication title  contains wmt and the nearest publication title in doras also contain wmt\n",
    "    def check_wmt(title, near_title):\n",
    "        try:\n",
    "            year1 = re.findall(\"wmt[0-9]{2}\",title.lower())[0][-2:]\n",
    "            year2 = re.findall(\"wmt[0-9]{2}\",near_title.lower())[0][-2:]\n",
    "        except IndexError:\n",
    "            return False\n",
    "        return True\n",
    "    # another method testing whether the year in a publications title containing \"wmt\" and the year in the nearest publication title in doras are the same \n",
    "    def equal_wmt(title,near_title):\n",
    "            year1 = re.findall(\"wmt[0-9]{2}\",title.lower())[0][-2:]\n",
    "            year2 = re.findall(\"wmt[0-9]{2}\",near_title.lower())[0][-2:]\n",
    "            return year1 == year2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # recounciling the doras profile verion and doras faculty page as there is publications that do not appear on both!\n",
    "    def reconcile_doras(value,doras_name,doras_df,doras_soc_df):\n",
    "        soc = doras_soc_df[doras_soc_df[\"Author List\"].str.contains(doras_name)]\n",
    "\n",
    "        soc = soc[[\n",
    "        'Publication Title',\n",
    "        'Author List',\n",
    "        'Conf/Journal Details',\n",
    "        'Year',\n",
    "        'ISBN',\n",
    "        'ISSN',\n",
    "        'Item Type',\n",
    "        'Event Type',\n",
    "        'Refereed',\n",
    "        'Date of Award',\n",
    "        'Supervisor(s)',\n",
    "        'Uncontrolled Keywords',\n",
    "        'Subject',\n",
    "        'DCU Faculties and Centres',\n",
    "        'Use License',\n",
    "        'ID Code',\n",
    "        'Deposited On',\n",
    "        'Published in',\n",
    "        'Publisher',\n",
    "        'Official URL',\n",
    "        'Copyright Information',\n",
    "        'Funders',\n",
    "        'Additional Information',\n",
    "        'Tweets',\n",
    "        'Mendeley Readers']]\n",
    "\n",
    "        soc.insert(0, 'Research name', value)\n",
    "\n",
    "\n",
    "        doras_df = doras_df[['Research name', 'Publication Title', 'Author List',\n",
    "        'Conf/Journal Details', 'Year', 'ISBN', 'ISSN', 'Item Type',\n",
    "        'Event Type', 'Refereed', 'Date of Award', 'Supervisor(s)',\n",
    "        'Uncontrolled Keywords', 'Subject', 'DCU Faculties and Centres',\n",
    "        'Use License', 'ID Code', 'Deposited On', 'Published in', 'Publisher',\n",
    "        'Official URL', 'Copyright Information', 'Funders',\n",
    "        'Additional Information', 'Tweets', 'Mendeley Readers']]\n",
    "\n",
    "        final = pd.concat([soc,doras_df] ,ignore_index=True)\n",
    "        final[\"Publication Title\"] = final[\"Publication Title\"].apply(lambda x: \" \".join(str(x).splitlines())) \n",
    "\n",
    "\n",
    "\n",
    "        final.drop_duplicates(inplace=True, ignore_index=True,keep=\"first\")\n",
    "        return final \n",
    "\n",
    "        \n",
    "\n",
    "    # through emperical analysis I found out  the  best suited standard Levenshtein distance similarity ratio, those not in the dictionary default to 90\n",
    "    dic = {\"Rob Brennan\": 92, \"Annalina Caputo\": 89, \"Long Cheng\":89, \"Jennifer Foster\":89, \"Jane Kernan\": 89,\"Alistair Sutherland\":88,\"Andy Way\":88,\"Murat YILMAZ\":87,\"Paul M. Clarke\":89,\"Gareth Jones\":87}\n",
    "    path = os.path.join(my_path, \"data/Doras SOC/doras_soc.csv\")\n",
    "    doras_soc_df =  pd.read_csv(path)\n",
    "    for index, value in researchers['Researcher'].items():\n",
    "        value = value.strip()# value is the researchers name\n",
    "        doras_df = \"\"\n",
    "        scholar_df = \"\"\n",
    "        filename = \"_\".join(value.split(\" \"))\n",
    "        doras_name = \"\" # the name of the researcher in doras profile \n",
    "        # need try and except as not every researcher has a doras page\n",
    "        try:\n",
    "            path = os.path.join(my_path, \"data/Doras publications/{}.csv\".format(filename))\n",
    "            doras_df1 = pd.read_csv(path)\n",
    "            doras_name = researchers.iloc[index,4].strip()\n",
    "            doras_df = reconcile_doras(value,doras_name,doras_df1,doras_soc_df)\n",
    "            doras_df = doras_df[doras_df[\"Conf/Journal Details\"].str.contains(\"preprint\", na=False) == False].reset_index(drop=True)# removing preprints\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        # need try and except as not every researcher has a doras page\n",
    "        try:\n",
    "            path = os.path.join(my_path, \"data/Google Scholar Publications/{}.csv\".format(filename))\n",
    "            scholar_df = pd.read_csv(path)\n",
    "            scholar_df = scholar_df[scholar_df[\"Conf/Journal Details\"].str.contains(\"preprint\", na=False) == False].reset_index(drop=True)# removing preprints\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        # testing if we have both profiles(doras and google scholar) present\n",
    "        if len(doras_df) > 0 and len(scholar_df) > 0:\n",
    "            try:\n",
    "                score = int(dic[value])\n",
    "            except KeyError:\n",
    "                score = 90\n",
    "            choices1 = doras_df[\"Publication Title\"].values.tolist()#adding all of the doras tiitles to a list\n",
    "            for index, title in scholar_df[\"Publication Title\"].items():\n",
    "                distance = process.extractOne(str(title.strip()),choices1)[-1]# the ratio score\n",
    "                nearest_title = process.extractOne(str(title.strip()),choices1)[0]# the nearest title based on the ratio\n",
    "                if distance >= score and distance < 99: # catching rogue publications \n",
    "                    if check_years(title,nearest_title) == True:\n",
    "                        if equal_year(title,nearest_title) == False:\n",
    "                            not_on_doras = pd.concat([not_on_doras,scholar_df.iloc[[index]]],ignore_index=True)\n",
    "                    elif \"NTCIR\" in title and \"NTCIR\" in nearest_title:\n",
    "                        if check_ncir == False:\n",
    "                            not_on_doras = pd.concat([not_on_doras,scholar_df.iloc[[index]]],ignore_index=True)\n",
    "                            \n",
    "                    elif \"shared task (SR\" in title and \"shared task (SR\" in nearest_title:\n",
    "                        if check_sr(title,nearest_title) == False:\n",
    "                            not_on_doras = pd.concat([not_on_doras,scholar_df.iloc[[index]]],ignore_index=True)\n",
    "                    elif check_wmt(title,nearest_title) == True:\n",
    "                        if equal_wmt(title,nearest_title) == False:\n",
    "                            not_on_doras = pd.concat([not_on_doras,scholar_df.iloc[[index]]],ignore_index=True)\n",
    "                            \n",
    "                            \n",
    "                    else:# ones I could not catch and were ambigious. put them in the mismatch dataframe\n",
    "                        if title == \"Universal dependencies 1.1\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Evaluation of coordination techniques in synchronous collaborative information retrieval\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Oracle-based training for phrase-based statistical machine translation\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Machine translation\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Results of the wmt18 metrics shared task: Both characters and embeddings achieve good performance\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Results of the wmt16 metrics shared task\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Interaction and engagement for information research and learning with lifelogging devices\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Multimedia for personal health and health care\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                        elif title == \"Streamrule: a nonmonotonic stream reasoning system for the semantic web\":\n",
    "                            add_mis_matches(value,title,nearest_title,distance,scholar_df[\"Year\"].loc[index])\n",
    "                    \n",
    "                \n",
    "                # adding google scholar publication titles that do not appear in doras\n",
    "                elif process.extractOne(str(title.strip()),choices1)[-1] < score:\n",
    "                    not_on_doras = pd.concat([not_on_doras,scholar_df.iloc[[index]]],ignore_index=True)\n",
    "        # if a researcher has no doras profile, adding all of his/her publications to not_on_doras            \n",
    "        elif len(scholar_df) > 0 and len(doras_df)  == 0:# if they dont have a doras profile putting all of their google scholar profile papers into the not on doras df\n",
    "            not_on_doras = pd.concat([not_on_doras,scholar_df],ignore_index=True)\n",
    "            \n",
    "                    \n",
    "\n",
    "        \n",
    "\n",
    "    d = {'Research name':list_researcher,'Mismatch Title':list_mis_match_title,'Nearest Title':list_nearest_title,'Score':list_score,'Year':list_year,\"Filename\":None,\"Ignore Y/N\":None}\n",
    "    mis_matches_df = pd.DataFrame(data=d)\n",
    "    path = os.path.join(my_path, \"data/Missing publications/mismatches_not_on_doras.csv\".format(filename))\n",
    "\n",
    "    mis_matches_df.to_csv(path, index = None, header=True)\n",
    "    path = os.path.join(my_path, \"data/Missing publications/not_on_doras.csv\")\n",
    "    not_on_doras[\"Filename\"] = None\n",
    "    not_on_doras[\"Ignore Y/N\"] = None\n",
    "    not_on_doras.to_csv(path, index = None, header=True)\n",
    "\n",
    "\n",
    "def not_on_google_scholar():\n",
    "    my_path = os.path.abspath(os.path.dirname(__file__))\n",
    "    path = os.path.join(my_path, \"data/SOC_Researchers_with_doras_names.csv\")\n",
    "    researchers = pd.read_csv(path,encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # a dataframe containing publications that are in a researcher's doras profile/faculty page but not on their google scholar profile \n",
    "    not_on_google_scholar =pd.DataFrame(columns=['Research name','Publication Title','Author List','Conf/Journal Details','Year','ISBN', 'ISSN', 'Item Type',\n",
    "        'Event Type', 'Refereed', 'Date of Award', 'Supervisor(s)',\n",
    "        'Uncontrolled Keywords', 'Subject', 'DCU Faculties and Centres',\n",
    "        'Use License', 'ID Code', 'Deposited On', 'Published in', 'Publisher',\n",
    "        'Official URL', 'Copyright Information', 'Funders',\n",
    "        'Additional Information', 'Tweets', 'Mendeley Readers'])\n",
    "    # datframe containing ambigious publications\n",
    "    mis_matches = pd.DataFrame(columns=['Research name','Mismatch Title','Nearest Title','Score','Year'])\n",
    "\n",
    "    list_mis_match_title = []\n",
    "    list_nearest_title  = []\n",
    "    list_score = []\n",
    "    list_researcher = []\n",
    "    list_year = []\n",
    "    # method that assists in the development of a dataframe that contains  mismatched publications\n",
    "    def add_mis_matches(research, title,near,score,year):\n",
    "        list_researcher.append(research)\n",
    "        list_mis_match_title.append(title)\n",
    "        list_nearest_title.append(near)\n",
    "        list_score.append(score)\n",
    "        list_year.append(year)\n",
    "\n",
    "    #checks if there is a year present in the publication title and the nearest publication title on google scholar\n",
    "    def check_years(title,near_title):\n",
    "        try:\n",
    "            year1 = re.findall(\"[2][0-9]{3}\",title)[0]\n",
    "            year2 = re.findall(\"[2][0-9]{3}\",near_title)[0]\n",
    "        except IndexError:\n",
    "            return False\n",
    "        return True\n",
    "    # tests if the year in a publications title and the year in  the nearest publication title on google scholar are the same\n",
    "    def equal_year(title,near_title):\n",
    "        year1 = re.findall(\"[2][0-9]{3}\",title)[0]\n",
    "        year2 = re.findall(\"[2][0-9]{3}\",near_title)[0]\n",
    "        return year1 == year2\n",
    "    #another method testing whether the year in a publications title containing \"NICIR\" and the year in the nearest publication title on google scholar are the same \n",
    "    def check_ncir(title,near_title):\n",
    "        year1 = re.findall(\"NTCIR-[0-9]{2}\",title)[0]\n",
    "        year2 = re.findall(\"NTCIR-[0-9]{2}\",near_title)[0]\n",
    "        return year1 == year2\n",
    "    ##another method testing whether the year in a publications title containing \"shared task (SR\" and the year in the nearest publication title on google scholar are the same \n",
    "    def check_sr(title, near_title):\n",
    "        year1 = re.findall(\"(SR[â€™|'][0-9]{2})\",title)[0][-2:]\n",
    "        year2 = re.findall(\"(SR[â€™|'][0-9]{2})\",near_title)[0][-2:]\n",
    "        return year1 == year2\n",
    "    # checks whether a  publication title  contains wmt and the nearest publication title in google scholar also contain wmt\n",
    "    def check_wmt(title, near_title):\n",
    "        try:\n",
    "            year1 = re.findall(\"wmt[0-9]{2}\",title.lower())[0][-2:]\n",
    "            year2 = re.findall(\"wmt[0-9]{2}\",near_title.lower())[0][-2:]\n",
    "        except IndexError:\n",
    "            return False\n",
    "        return True\n",
    "    # another method testing whether the year in a publications title containing \"wmt\" and the year in the nearest publication title in  are the same \n",
    "    def equal_wmt(title,near_title):\n",
    "            year1 = re.findall(\"wmt[0-9]{2}\",title.lower())[0][-2:]\n",
    "            year2 = re.findall(\"wmt[0-9]{2}\",near_title.lower())[0][-2:]\n",
    "            return year1 == year2\n",
    "        \n",
    "    # recounciling the doras profile verion and doras faculty page as there is publications that do not appear on both!\n",
    "    def reconcile_doras(value,doras_name,doras_df,doras_soc_df):\n",
    "        soc = doras_soc_df[doras_soc_df[\"Author List\"].str.contains(doras_name)]\n",
    "\n",
    "        soc = soc[[\n",
    "        'Publication Title',\n",
    "        'Author List',\n",
    "        'Conf/Journal Details',\n",
    "        'Year',\n",
    "        'ISBN',\n",
    "        'ISSN',\n",
    "        'Item Type',\n",
    "        'Event Type',\n",
    "        'Refereed',\n",
    "        'Date of Award',\n",
    "        'Supervisor(s)',\n",
    "        'Uncontrolled Keywords',\n",
    "        'Subject',\n",
    "        'DCU Faculties and Centres',\n",
    "        'Use License',\n",
    "        'ID Code',\n",
    "        'Deposited On',\n",
    "        'Published in',\n",
    "        'Publisher',\n",
    "        'Official URL',\n",
    "        'Copyright Information',\n",
    "        'Funders',\n",
    "        'Additional Information',\n",
    "        'Tweets',\n",
    "        'Mendeley Readers']]\n",
    "\n",
    "        soc.insert(0, 'Research name', value)\n",
    "\n",
    "\n",
    "        doras_df = doras_df[['Research name', 'Publication Title', 'Author List',\n",
    "        'Conf/Journal Details', 'Year', 'ISBN', 'ISSN', 'Item Type',\n",
    "        'Event Type', 'Refereed', 'Date of Award', 'Supervisor(s)',\n",
    "        'Uncontrolled Keywords', 'Subject', 'DCU Faculties and Centres',\n",
    "        'Use License', 'ID Code', 'Deposited On', 'Published in', 'Publisher',\n",
    "        'Official URL', 'Copyright Information', 'Funders',\n",
    "        'Additional Information', 'Tweets', 'Mendeley Readers']]\n",
    "\n",
    "        final = pd.concat([soc,doras_df] ,ignore_index=True)\n",
    "        final[\"Publication Title\"] = final[\"Publication Title\"].apply(lambda x: \" \".join(str(x).splitlines()))\n",
    "\n",
    "\n",
    "\n",
    "        final.drop_duplicates(inplace=True, ignore_index=True, keep=\"first\")\n",
    "        return final \n",
    "\n",
    "    # through emperical analysis I found out  the  best suited standard Levenshtein distance similarity ratio, those not in the dictionary default to 90\n",
    "    dic = {\"Suzanne Little\":92}\n",
    "    path = os.path.join(my_path, \"data/Doras SOC/doras_soc.csv\")\n",
    "    doras_soc_df =  pd.read_csv(path)\n",
    "\n",
    "    for index, value in researchers['Researcher'].items():\n",
    "        value = value.strip()# value is the researchers name\n",
    "        doras_df = \"\"\n",
    "        scholar_df = \"\"\n",
    "        filename = \"_\".join(value.split(\" \"))\n",
    "        doras_name = \"\" # the name of the researcher in doras profile \n",
    "        # need try and except as not every researcher has a doras page\n",
    "        try:\n",
    "            path = os.path.join(my_path, \"data/Doras publications/{}.csv\".format(filename))\n",
    "            doras_df1 = pd.read_csv(path)\n",
    "            doras_name = researchers.iloc[index,4].strip()\n",
    "            doras_df = reconcile_doras(value,doras_name,doras_df1,doras_soc_df)\n",
    "            doras_df = doras_df[doras_df[\"Conf/Journal Details\"].str.contains(\"preprint\", na=False) == False].reset_index(drop=True)# removing preprints\n",
    "        \n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        # need try and except as not every researcher has a doras page\n",
    "        try:\n",
    "            path = os.path.join(my_path, \"data/Google Scholar Publications/{}.csv\".format(filename))\n",
    "            scholar_df = pd.read_csv(path)\n",
    "            scholar_df = scholar_df[scholar_df[\"Conf/Journal Details\"].str.contains(\"preprint\", na=False) == False].reset_index(drop=True)# r\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "        # testing if we have both profiles(doras and google scholar) present\n",
    "        if len(doras_df) > 0 and len(scholar_df) > 0:\n",
    "            try:\n",
    "                score = int(dic[value])\n",
    "            except KeyError:\n",
    "                score = 90\n",
    "            choices1 = scholar_df[\"Publication Title\"].values.tolist() #adding all of the google scholar tiitles to a list\n",
    "            for index, title in doras_df[\"Publication Title\"].items():\n",
    "                distance = process.extractOne(str(title.strip()),choices1)[-1] # the ratio score\n",
    "                nearest_title = process.extractOne(str(title.strip()),choices1)[0] #the nearest title based on the ratio\n",
    "                if distance >= score and distance < 99:\n",
    "                    if check_years(title,nearest_title) == True:\n",
    "                        if equal_year(title,nearest_title) == False:\n",
    "                            not_on_google_scholar = pd.concat([not_on_google_scholar,doras_df.iloc[[index]]],ignore_index=True)\n",
    "                    elif \"NTCIR\" in title and \"NTCIR\" in nearest_title:\n",
    "                        if check_ncir == False:\n",
    "                            not_on_google_scholar = pd.concat([not_on_google_scholar,doras_df.iloc[[index]]],ignore_index=True)\n",
    "                    elif \"shared task (SR\" in title and \"shared task (SR\" in nearest_title:\n",
    "                        if check_sr(title,nearest_title) == False:\n",
    "                            not_on_google_scholar = pd.concat([not_on_google_scholar,doras_df.iloc[[index]]],ignore_index=True)\n",
    "                    elif check_wmt(title,nearest_title) == True:\n",
    "                        if equal_wmt(title,nearest_title) == False:\n",
    "                            not_on_google_scholar = pd.concat([not_on_google_scholar,doras_df.iloc[[index]]],ignore_index=True)\n",
    "                                    \n",
    "                # adding google  titles that do not appear in google scholar\n",
    "                if process.extractOne(str(title.strip()),choices1)[-1] < score:\n",
    "                    not_on_google_scholar = pd.concat([not_on_google_scholar,doras_df.iloc[[index]]],ignore_index=True)\n",
    "                    \n",
    "        elif len(scholar_df) == 0 and len(doras_df)  > 0:# if they dont have a google scholar profile putting all of their doras papers into the not on google scholar df\n",
    "            not_on_google_scholar = pd.concat([not_on_google_scholar,doras_df],ignore_index=True)\n",
    "            \n",
    "                    \n",
    "\n",
    "        \n",
    "\n",
    "    d = {'Research name':list_researcher,'Mismatch Title':list_mis_match_title,'Nearest Title':list_nearest_title,'Score':list_score,\"Year\":list_year}\n",
    "    mis_matches_df = pd.DataFrame(data=d)\n",
    "    path = os.path.join(my_path, \"data/Missing publications/mismatches_not_on_google_scholar.csv\")\n",
    "    mis_matches_df.to_csv(path, index = None, header=True)\n",
    "    path = os.path.join(my_path, \"data/Missing publications/not_on_google_scholar.csv\")\n",
    "\n",
    "\n",
    "    not_on_google_scholar.to_csv(path, index = None, header=True)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def individual_missing():\n",
    "    my_path = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "    path = os.path.join(my_path, \"data/Missing publications/not_on_google_scholar.csv\")\n",
    "    scholar_df = pd.read_csv(path)\n",
    "    \n",
    "    path = os.path.join(my_path, \"data/Missing publications/not_on_doras.csv\")\n",
    "    doras_df = pd.read_csv(path)\n",
    "    idx = np.random.permutation(np.arange(len(doras_df)))\n",
    "    doras_df = doras_df.iloc[idx].drop_duplicates(subset=[\"Publication Title\"],ignore_index=True,keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "\n",
    "    idx = np.random.permutation(np.arange(len(scholar_df)))\n",
    "    scholar_df = scholar_df.iloc[idx].drop_duplicates(subset=[\"Publication Title\"],ignore_index=True,keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    for value in  scholar_df[\"Research name\"].unique():\n",
    "        df = scholar_df[scholar_df[\"Research name\"] == value]\n",
    "        filename = \"_\".join(value.split(\" \"))\n",
    "        path = os.path.join(my_path, r\"data/Individual Missing Publications/Not on google scholar/{}.csv\".format(filename))\n",
    "        df.to_csv(path, index = None, header=True)\n",
    "\n",
    "    for value in  doras_df[\"Research name\"].unique():\n",
    "        df = doras_df[doras_df[\"Research name\"] == value]\n",
    "        filename = \"_\".join(value.split(\" \"))\n",
    "        path = os.path.join(my_path, r\"data/Individual Missing Publications/Not on doras/{}.csv\".format(filename))\n",
    "        df.to_csv(path ,index = None, header=True)\n",
    "\n",
    "\n",
    "def run_scrapers_and_populate_csv_folders():\n",
    "    google_scholar_scraper(2017,)\n",
    "    get_doras_dcu(2017,2019)\n",
    "    get_doras_researcher(2017,2019)\n",
    "    not_on_doras()\n",
    "    not_on_google_scholar()\n",
    "    individual_missing()\n",
    "    \n",
    "    \n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"postgresql://u:p@host/database\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ll\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    missing_doras_df = pd.read_sql(\"SELECT * FROM missing_doras_publications where ignore=False or ignore=null\", 'postgresql://postgres:socadmin@ec2-34-242-162-184.eu-west-1.compute.amazonaws.com:5432/citation_plan').drop([\"index\"],axis=1, errors='ignore')\n",
    "except:\n",
    "    print(\"ll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Research name</th>\n",
       "      <th>Publication Title</th>\n",
       "      <th>Author List</th>\n",
       "      <th>Conf/Journal Details</th>\n",
       "      <th>Citation count</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alan F. Smeaton</td>\n",
       "      <td>Optimised Convolutional Neural Networks for He...</td>\n",
       "      <td>E Brophy, W Muehlhausen, AF Smeaton, TE Ward</td>\n",
       "      <td>arXiv preprint arXiv:2004.00505</td>\n",
       "      <td>0</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alan F. Smeaton</td>\n",
       "      <td>MultiMWE: Building a Multi-lingual Multi-Word ...</td>\n",
       "      <td>L Han, GJF Jones, AF Smeaton</td>\n",
       "      <td>Proceedings of The 12th Language Resources and...</td>\n",
       "      <td>0</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alan F. Smeaton</td>\n",
       "      <td>Interpreting CNN for Low Complexity Learned Su...</td>\n",
       "      <td>L Murn, S Blasi, AF Smeaton, NE O'Connor, M Mrak</td>\n",
       "      <td>arXiv preprint arXiv:2006.06392</td>\n",
       "      <td>0</td>\n",
       "      <td>2020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alan F. Smeaton</td>\n",
       "      <td>Assistive technology: Understanding the needs ...</td>\n",
       "      <td>SJ Oâ€™Neill, S Smyth, A Smeaton, NE Oâ€™Connor</td>\n",
       "      <td>Assistive Technology 109</td>\n",
       "      <td>4</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan F. Smeaton</td>\n",
       "      <td>Neuroscore: A Brain-inspired Evaluation Metric...</td>\n",
       "      <td>Z Wang, Q She, AF Smeaton, TE Ward, G Healy</td>\n",
       "      <td>arXiv preprint arXiv:1905.04243</td>\n",
       "      <td>8</td>\n",
       "      <td>2019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1391</th>\n",
       "      <td>Yvette Graham</td>\n",
       "      <td>Crowd-Sourcing of Human Judgments of Machine T...</td>\n",
       "      <td>Y Graham, T Baldwin, A Moffat, J Zobel</td>\n",
       "      <td>Proceedings of the Australasian Language Techn...</td>\n",
       "      <td>1</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>Yvette Graham</td>\n",
       "      <td>A Dependency-Constrained Hierarchical Model wi...</td>\n",
       "      <td>Y Graham</td>\n",
       "      <td>Proceedings of the Eighth Workshop on Statisti...</td>\n",
       "      <td>1</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>Yvette Graham</td>\n",
       "      <td>Measurement of progress in machine translation</td>\n",
       "      <td>Y Graham, T Baldwin, A Harwood, A Moffat, J Zobel</td>\n",
       "      <td>Proceedings of the Australasian Language Techn...</td>\n",
       "      <td>9</td>\n",
       "      <td>2012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>Yvette Graham</td>\n",
       "      <td>Sulis: An Open Source Transfer Decoder for Dee...</td>\n",
       "      <td>Y Graham</td>\n",
       "      <td>The Prague Bulletin of Mathematical Linguistic...</td>\n",
       "      <td>4</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>Yvette Graham</td>\n",
       "      <td>Factor templates for factored machine translat...</td>\n",
       "      <td>Y Graham, J Genabith</td>\n",
       "      <td>International Workshop on Spoken Language Tran...</td>\n",
       "      <td>9</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1396 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Research name                                  Publication Title  \\\n",
       "0     Alan F. Smeaton  Optimised Convolutional Neural Networks for He...   \n",
       "1     Alan F. Smeaton  MultiMWE: Building a Multi-lingual Multi-Word ...   \n",
       "2     Alan F. Smeaton  Interpreting CNN for Low Complexity Learned Su...   \n",
       "3     Alan F. Smeaton  Assistive technology: Understanding the needs ...   \n",
       "4     Alan F. Smeaton  Neuroscore: A Brain-inspired Evaluation Metric...   \n",
       "...               ...                                                ...   \n",
       "1391    Yvette Graham  Crowd-Sourcing of Human Judgments of Machine T...   \n",
       "1392    Yvette Graham  A Dependency-Constrained Hierarchical Model wi...   \n",
       "1393    Yvette Graham     Measurement of progress in machine translation   \n",
       "1394    Yvette Graham  Sulis: An Open Source Transfer Decoder for Dee...   \n",
       "1395    Yvette Graham  Factor templates for factored machine translat...   \n",
       "\n",
       "                                            Author List  \\\n",
       "0          E Brophy, W Muehlhausen, AF Smeaton, TE Ward   \n",
       "1                          L Han, GJF Jones, AF Smeaton   \n",
       "2      L Murn, S Blasi, AF Smeaton, NE O'Connor, M Mrak   \n",
       "3           SJ Oâ€™Neill, S Smyth, A Smeaton, NE Oâ€™Connor   \n",
       "4           Z Wang, Q She, AF Smeaton, TE Ward, G Healy   \n",
       "...                                                 ...   \n",
       "1391             Y Graham, T Baldwin, A Moffat, J Zobel   \n",
       "1392                                           Y Graham   \n",
       "1393  Y Graham, T Baldwin, A Harwood, A Moffat, J Zobel   \n",
       "1394                                           Y Graham   \n",
       "1395                               Y Graham, J Genabith   \n",
       "\n",
       "                                   Conf/Journal Details  Citation count  \\\n",
       "0                       arXiv preprint arXiv:2004.00505               0   \n",
       "1     Proceedings of The 12th Language Resources and...               0   \n",
       "2                       arXiv preprint arXiv:2006.06392               0   \n",
       "3                              Assistive Technology 109               4   \n",
       "4                       arXiv preprint arXiv:1905.04243               8   \n",
       "...                                                 ...             ...   \n",
       "1391  Proceedings of the Australasian Language Techn...               1   \n",
       "1392  Proceedings of the Eighth Workshop on Statisti...               1   \n",
       "1393  Proceedings of the Australasian Language Techn...               9   \n",
       "1394  The Prague Bulletin of Mathematical Linguistic...               4   \n",
       "1395  International Workshop on Spoken Language Tran...               9   \n",
       "\n",
       "        Year  \n",
       "0     2020.0  \n",
       "1     2020.0  \n",
       "2     2020.0  \n",
       "3     2019.0  \n",
       "4     2019.0  \n",
       "...      ...  \n",
       "1391  2013.0  \n",
       "1392  2013.0  \n",
       "1393  2012.0  \n",
       "1394  2010.0  \n",
       "1395  2010.0  \n",
       "\n",
       "[1396 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_doras_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ambigious = pd.read_sql_table(\"ambigious_missing_doras_publications\", 'postgresql://postgres:socadmin@ec2-34-242-162-184.eu-west-1.compute.amazonaws.com:5432/citation_plan').drop([\"index\"],axis=1, errors='ignore').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Research name</th>\n",
       "      <th>Mismatch Title</th>\n",
       "      <th>Nearest Title</th>\n",
       "      <th>Score</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jennifer Foster</td>\n",
       "      <td>Universal dependencies 1.1</td>\n",
       "      <td>Universal dependencies for Irish.</td>\n",
       "      <td>91</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cathal Gurrin</td>\n",
       "      <td>Multimedia for personal health and health care</td>\n",
       "      <td>MMHealth 2017: Workshop on Multimedia for pers...</td>\n",
       "      <td>90</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cathal Gurrin</td>\n",
       "      <td>Interaction and engagement for information res...</td>\n",
       "      <td>Second interaction and engagement on informati...</td>\n",
       "      <td>93</td>\n",
       "      <td>2016.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alessandra Mileo</td>\n",
       "      <td>Streamrule: a nonmonotonic stream reasoning sy...</td>\n",
       "      <td>Stream Reasoning.</td>\n",
       "      <td>90</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alan F. Smeaton</td>\n",
       "      <td>Evaluation of coordination techniques in synch...</td>\n",
       "      <td>Synchronous collaborative information retrieva...</td>\n",
       "      <td>92</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alan F. Smeaton</td>\n",
       "      <td>Evaluation of coordination techniques in synch...</td>\n",
       "      <td>Synchronous collaborative information retrieva...</td>\n",
       "      <td>92</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Andy Way</td>\n",
       "      <td>Oracle-based training for phrase-based statist...</td>\n",
       "      <td>Accuracy-based scoring for phrase-based statis...</td>\n",
       "      <td>89</td>\n",
       "      <td>2011.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Andy Way</td>\n",
       "      <td>Machine translation</td>\n",
       "      <td>Multiple segmentations of Thai sentences for n...</td>\n",
       "      <td>90</td>\n",
       "      <td>2010.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Research name                                     Mismatch Title  \\\n",
       "0   Jennifer Foster                         Universal dependencies 1.1   \n",
       "1     Cathal Gurrin     Multimedia for personal health and health care   \n",
       "2     Cathal Gurrin  Interaction and engagement for information res...   \n",
       "3  Alessandra Mileo  Streamrule: a nonmonotonic stream reasoning sy...   \n",
       "4   Alan F. Smeaton  Evaluation of coordination techniques in synch...   \n",
       "5   Alan F. Smeaton  Evaluation of coordination techniques in synch...   \n",
       "6          Andy Way  Oracle-based training for phrase-based statist...   \n",
       "7          Andy Way                                Machine translation   \n",
       "\n",
       "                                       Nearest Title  Score    Year  \n",
       "0                  Universal dependencies for Irish.     91  2015.0  \n",
       "1  MMHealth 2017: Workshop on Multimedia for pers...     90  2016.0  \n",
       "2  Second interaction and engagement on informati...     93  2016.0  \n",
       "3                                  Stream Reasoning.     90  2013.0  \n",
       "4  Synchronous collaborative information retrieva...     92  2009.0  \n",
       "5  Synchronous collaborative information retrieva...     92  2009.0  \n",
       "6  Accuracy-based scoring for phrase-based statis...     89  2011.0  \n",
       "7  Multiple segmentations of Thai sentences for n...     90  2010.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_doras = pd.read_sql_table(\"missing_doras_publications\", 'postgresql://postgres:socadmin@ec2-34-242-162-184.eu-west-1.compute.amazonaws.com:5432/citation_plan').drop([\n",
    "        \"index\"], axis=1, errors='ignore').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_doras = pd.read_sql_table(\"missing_doras_publications\", 'postgresql+psycopg2://postgres:socadmin@ec2-34-242-162-184.eu-west-1.compute.amazonaws.com:5432/citation_plan').drop([\"index\"], axis=1, errors='ignore').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_doras = pd.read_sql_table(\"missing_doras_publications\", 'postgresql://postgres:socadmin@ec2-34-242-162-184.eu-west-1.compute.amazonaws.com:5432/citation_plan').drop([\"index\"], axis=1, errors='ignore').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
